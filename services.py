import os
from dotenv import load_dotenv
from langchain_cohere import CohereEmbeddings, ChatCohere
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

# --- CONFIGURACI칍N E INICIALIZACI칍N ---

# Ruta donde guardaste la base de datos (Aseg칰rate de que coincida con tu script de creaci칩n)
DB_PATH = "./chroma_db_cohere"

# Verificaci칩n de seguridad
load_dotenv()
api_key = os.getenv("COHERE_API_KEY")

# 1. Inicializar Modelos (Se cargan una sola vez al importar el archivo)

# A. Modelo de Embeddings (Para buscar en la base de datos)
embedding_model = CohereEmbeddings(model="embed-multilingual-v3.0")

# B. Cargar Base de Datos Vectorial
if not os.path.exists(DB_PATH):
    raise FileNotFoundError(f"丘멆잺 No se encuentra la base de datos en {DB_PATH}. Ejecuta primero el script de ingesti칩n.")

vectorstore = Chroma(
    persist_directory=DB_PATH,
    embedding_function=embedding_model
)

# C. Definir los LLMs (Cerebro R치pido vs Cerebro Potente)
llm_rapido = ChatCohere(model="command-r-08-2024", temperature=0, max_tokens=5)
llm_potente = ChatCohere(model="command-r-08-2024", temperature=0)


# --- FUNCIONES DE SERVICIO ---

def clasificar_intencion(pregunta: str) -> str:
    """
    Usa el modelo R츼PIDO para decidir qu칠 quiere hacer el usuario.
    Categor칤as:
    - SALUDO: Hola, gracias, chau, buen d칤a.
    - BUSQUEDA: Preguntas sobre recetas, comida, ingredientes.
    - OFF_TOPIC: Pol칤tica, deportes, c칩digo, mec치nica.
    """
    prompt_router = PromptTemplate.from_template(
        """Eres el cerebro de un asistente de cocina. Clasifica la siguiente frase del usuario en una de estas 3 categor칤as:

        1. "SALUDO": Si el usuario saluda, agradece, se despide o hace charla casual (ej: "Hola", "Qui칠n sos").
        2. "BUSQUEDA": Si el usuario pregunta sobre recetas, comida, historia culinaria o ingredientes.
        3. "OFF_TOPIC": Si el usuario habla de temas que NO son cocina ni saludos (ej: f칰tbol, programaci칩n).

        Usuario: {input}

        Responde 칔NICAMENTE con una palabra: SALUDO, BUSQUEDA u OFF_TOPIC."""
    )

    chain = prompt_router | llm_rapido | StrOutputParser()
    
    # Limpiamos la salida para evitar espacios o puntos extra
    intencion = chain.invoke({"input": pregunta}).strip().upper()
    return intencion


def responder_charla_casual(pregunta: str) -> str:
    """
    Responde saludos o preguntas personales sin buscar en la base de datos.
    Usa el modelo r치pido para ser veloz.
    """
    prompt_chat = PromptTemplate.from_template(
        """Eres "ChefBot", un asistente experto en gastronom칤a argentina amigable y servicial.
        El usuario te ha dicho: "{input}"
        
        Respondele de forma breve, carism치tica y siempre invit치ndolo a cocinar o preguntar por una receta.
        NO inventes recetas, solo charla."""
    )
    
    chain = prompt_chat | llm_rapido | StrOutputParser()
    return chain.invoke({"input": pregunta})


def buscar_contexto(pregunta: str, k: int = 5):
    """
    Realiza el embedding de la consulta y busca los 'k' fragmentos 
    m치s similares en la base de datos vectorial.
    """
    # Buscamos directamente los documentos
    docs = vectorstore.similarity_search(pregunta, k=k)
    return docs


def validar_fidelidad(pregunta: str, respuesta: str, contexto: str) -> bool:
    """
    Funci칩n Juez: Verifica si la respuesta generada est치 realmente
    respaldada por el contexto recuperado del PDF.
    Retorna: True (Aprobado) / False (Alucinaci칩n)
    """
    
    prompt_juez = ChatPromptTemplate.from_template(
        """Eres un auditor de calidad estricto. Tu trabajo es detectar alucinaciones en un sistema RAG.
        
        1. Analiza el CONTEXTO y la RESPUESTA.
        2. Si la respuesta contiene informaci칩n que NO est치 en el contexto, es una alucinaci칩n (responde NO).
        3. Si la respuesta contradice el contexto, responde NO.
        4. Si la respuesta se basa fielmente en el contexto, responde SI.

        CONTEXTO:
        {contexto}

        PREGUNTA USUARIO: {pregunta}
        RESPUESTA BOT: {respuesta}

        VEREDICTO (Solo responde SI o NO):"""
    )

    # Usamos llm_rapido porque es bueno clasificando y muy veloz
    chain = prompt_juez | llm_rapido | StrOutputParser()
    
    # Ejecutamos la auditor칤a
    veredicto = chain.invoke({
        "contexto": contexto,
        "pregunta": pregunta,
        "respuesta": respuesta
    })

    veredicto_limpio = veredicto.strip().upper()
    print(f"丘뒲잺  Juez de Fidelidad dice: {veredicto_limpio}")
    
    return "SI" in veredicto_limpio


def generar_respuesta_rag(pregunta: str) -> str:
    """
    Usa el modelo potente (Command R) para responder bas치ndose 
    en la informaci칩n recuperada de la base de datos.
    """
    
    # 1. Recuperar contexto (usamos la funci칩n anterior)
    docs = buscar_contexto(pregunta)
    contexto_texto = "\n\n".join([d.page_content for d in docs])
    
    # 2. Si no hay contexto relevante (opcional, pero buena pr치ctica)
    if not contexto_texto:
        return {
            "respuesta": "No encontr칠 informaci칩n en el libro sobre eso.",
            "validado": False
        }
    
    # 3. Crear el Prompt
    template = """Eres un experto Chef Argentino. Responde la pregunta del usuario bas치ndote EXCLUSIVAMENTE en el siguiente contexto extra칤do del libro 'Gastronom칤a Regional Argentina'.
    
    - Si la respuesta no est치 en el contexto, di amablemente que no tienes esa informaci칩n.
    - Cita el nombre de la receta si aplica.
    - S칠 claro y did치ctico.
    - No se deben usar emojis en las respuestas.
    - Las respuestas deben ser siempre en espa침ol, independientemente del idioma de la pregunta.

    CONTEXTO:
    {context}

    PREGUNTA DEL USUARIO: 
    {question}

    RESPUESTA:"""

    prompt = ChatPromptTemplate.from_template(template)

    # 4. Cadena de generaci칩n
    chain = prompt | llm_potente | StrOutputParser()

    # 5. Ejecutar
    respuesta_candidata = chain.invoke({
        "context": contexto_texto,
        "question": pregunta
    })
    
    es_fiel = validar_fidelidad(pregunta, respuesta_candidata, contexto_texto)

    if es_fiel:
        return {
            "respuesta": respuesta_candidata,
            "validado": True
        }
    
    else:
        # Si el juez dice que es mentira, agregamos una advertencia visible
        advertencia = "\n\n丘멆잺 **Nota del Sistema:** Esta respuesta podr칤a contener informaci칩n externa no verificada en el libro original."
        return {
            "respuesta": respuesta_candidata + advertencia,
            "validado": False
        }


def orquestador_conversacional(pregunta: str) -> dict:
    """
    Funci칩n maestra que dirige el tr치fico.
    Retorna un diccionario con la respuesta y el tipo de acci칩n que tom칩.
    """
    # 1. El Orquestador decide
    intencion = clasificar_intencion(pregunta)
    print(f"游뱄 Orquestador detect칩 intenci칩n: {intencion}")

    # 2. Ejecutar acci칩n seg칰n la decisi칩n
    if intencion == "SALUDO":
        respuesta = responder_charla_casual(pregunta)
        return {
            "respuesta": respuesta, 
            "intencion": "Saludo 游눫",
            "validado": True
        }
    
    elif intencion == "BUSQUEDA":
        # Llamamos a tu funci칩n RAG existente (la que ya ten칤as)
        resultado_rag = generar_respuesta_rag(pregunta)
        return {"respuesta": resultado_rag["respuesta"], 
                "intencion": "Consulta gastronomica 游닀",
                "validado": resultado_rag["validado"]
        }
    
    else: # OFF_TOPIC
        return {
            "respuesta": "Lo siento, mi delantal es solo para cocinar. Preguntame sobre empanadas, locro o postres argentinos.", 
            "intencion": "Fuera de tema 游뛂",
            "validado": True
        }